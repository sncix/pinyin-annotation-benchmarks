# [草稿] 利用 python-pinyin + LLM 給多音字標註拼音的實驗

這裏介紹我最近做的一個嘗試：利用 [python-pinyin](https://github.com/mozillazg/python-pinyin) 和大語言模型（LLM）給多音字標註拼音。

## 前言

想要回答的是一個很簡單的問題：對於漢字（多音字）拼音的自動標註，現有技術能達到怎樣的準確度？

首先經過簡單的查找，找到了 python-pinyin 這個很不錯的漢字拼音轉換工具（PyPI 上的包名爲 pypinyin）。python-pinyin 預置了一些詞語讀音數據，如果是已知的詞語，可以直接給出預置的讀音；對於不在預置數據中的詞，則在啓用 `heteronym=True` 選項時給出所有可能的讀音。例如，假設預置數據不包含短語「中心」的讀音，那麼對於多音字「中」，python-pinyin 會給出所有可能的讀音 `['zhōng', 'zhòng']`：

```python
>>> from pypinyin import pinyin
>>> pinyin('中心', heteronym=True)
[['zhōng', 'zhòng'], ['xīn']]
```

再舉一例，假設預置數據中包含短語「目的」，且唯一讀音爲 `mù dì`，則無論是否啓用 `heteronym=True`，都只會得到唯一讀音。

```python
>>> from pypinyin import pinyin
>>> pinyin('目的', heteronym=True)
[['mù'], ['dì']]
```

不妨就將 python-pinyin 作爲本次實驗的起點。對於預置數據尚未涵蓋的短語中的多音字，常見有「取最常用的讀音，容許錯誤」和「人工校對」兩種處理方法。近年來，也有新穎的基於各類算法的方法，例如 [g2pW](https://github.com/GitYCC/g2pW/)（基於 BERT），已經獲得了非常好的準確率。但是这些方法的技術門檻較高（我的主觀感受），而現在有了具有對話能力的生成式語言模型，一個自然的想法是嘗試讓這樣的生成式模型根據指令選擇合適的讀音。本次實驗的內容就是驗證這個想法是否可行。

## 方法

### 獲取含多音字短語

[朙月拼音](https://github.com/rime/rime-luna-pinyin/) 是一個詞庫基於傳統字形的拼音輸入方案。我們從其詞庫中選取一些含有多音字的短語，使用人工智能進行標註；假設「詞庫中的讀音」近乎等同於「這些短語的全部正確讀音」，那麼通過將標註結果和原有讀音對比，就可以很方便地評估標註的準確程度。（本次隨緣選了「乾」、「彈」、「惡」、「樂」、「率」五個字，總共一千多個短語。）爲了方便讀者重複實驗，在 [pinyin-annotation](https://github.com/sncix/pinyin-annotation) 倉庫的 README 裏有這一步的示例腳本，可自行修改取用。

### 進行測試

由於 python-pinyin 內置的詞語讀音以簡體字爲主，在實際應用中，通常會將待處理內容先轉換爲簡體（例如用 [OpenCC](https://github.com/BYVoid/OpenCC) 等工具），再標註拼音，以充分利用其預置讀音數據。然而本實驗旨在多獲取數據來評估 LLM 的能力，故反其道而行之：有意不轉換爲簡體，以儘量少用 python-pinyin 的預置讀音數據，而多讓 LLM 根據上下文來選擇讀音。

實驗中測試的部分模型，例如 QwQ、OpenThinker 和 DeepSeek-R1-Distill-Qwen 爲具有推理能力的模型（reasoning model）。爲了充分利用各模型的能力，經過一些前期的嘗試後，選定了 2-pass 方法，亦即先讓模型自由進行推理，輸出非結構化的文本，再讓模型總結先前的推理，以 JSON 格式輸出結構化的結果。

詳細的代碼實現，歡迎參閱 <https://github.com/sncix/pinyin-annotation/tree/v0.1.0>。如果你有不同的方法，也非常歡迎介紹。

目前的程序有一個限制：暫時無法正確處理像「[善善惡惡](https://dict.revised.moe.edu.tw/dictView.jsp?ID=130984)」（讀作 shàn shàn wù è）這樣的短語中一個多音字出現多次的情形。不過這類短語數量較少，對計算結果影響不大。如果有後續實驗，可能會嘗試解決這個問題（比如需要標註哪個字，就將那個字用括號括起來）。

本次實驗使用 ollama（0.6.5 版本）運行大語言模型。涉及的模型如下所示：

- QwQ 32B
- OpenThinker 32B
- DeepSeek-R1-Distill-Qwen 32B
- Qwen2.5 32B
- Mistral Small 3.1 24B

各模型均使用由 ollama 官方分發的 Q4_K_M 量化版本[^1]。

[^1]: 使用的各模型在 ollama 內的 ID（實質上是 manifest 文件的 sha256 摘要值的前十二位，雖然 ollama 官方文檔未見記載這一點）爲：qwq:32b 009cb3f08d74；openthinker:32b 04b5937dcb16；deepseek-r1:32b 0a8c26691023；qwen2.5:32b 9f13ba1299af；mistral-small3.1:24b b9aaf0c2586a。

## 結果

各模型與現有詞庫讀音不一致數見下表所示（左欄括號內爲含有該字的短語總數）。數字越小，表示和現有詞庫符合度越好。

|               | qwq:32b | openthinker:32b | deepseek-r1:32b | qwen2.5:32b | mistral-small3.1:24b |
|---------------|--------:|----------------:|----------------:|------------:|---------------------:|
| 乾 (225[^2])  |      36 |              49 |              66 |          42 |                  151 |
| 彈 (207)      |      28 |              56 |              68 |          51 |                   64 |
| 惡  (66)      |      27 |              30 |              31 |          31 |                   29 |
| 樂 (479)      |     105 |             118 |             124 |         134 |                  230 |
| 率 (201[^3])  |      18 |              30 |              39 |          38 |                   47 |

[^2]: 在實驗所用的 pypinyin 0.54.0 中，「沒」只有 `méi` 這一個讀音，因此無法爲詞語「[乾沒](https://dict.revised.moe.edu.tw/dictView.jsp?ID=69865)」（正確的讀音爲 `gān mò`）選擇正確的讀音，故從計算中排除。（已在 <https://github.com/mozillazg/pinyin-data/issues/55> 反饋。）

[^3]: 在實驗所用的 pypinyin 0.54.0 中，「放大率」會被錯誤轉換爲 `fàng dà shuài`，因此從計算中排除。（此爲 pypinyin 內置數據的 bug，已通過 <https://github.com/mozillazg/phrase-pinyin-data/pull/56> 修復。）

詳細實驗結果可在 [collected_results_20250408/](collected_results_20250408/) 路徑下查閱，其中 log 文件爲程序的運行日誌（包含 LLM 的全部輸出），txt 文件爲各短語標註後的讀音。

### 一句話總結

本次實驗共測試了五種 32B（Q4_K_M）及以下級別的 LLM，在拼音標註方面都未達能夠實用的水準，但不知是模型能力如此，還是使用方法未能發揮其潛力。相對來說，QwQ 的表現最好。

## 倫理探討

最後一節中，我們簡略探討一些本實驗涉及到的倫理問題。注意 AI 領域的法律與政策仍然處於較快變化之中，各地域間也不同，更何況我知之甚少，因此這裏將會側重於一般原理，而非實際的法律規定。提及的概念除非明記，都不應視爲法律上的嚴格概念。

### 同意與授權

首先是權利的歸屬問題。生成式 AI 通常會涉及到三種權利對象：訓練所使用的數據集、訓練完成後的模型、及使用模型生成的輸出。其中前兩者常常能通過既有的事物類推加以理解（這並不意味着有關它們的問題都有確定的回答），而有關運行輸出的問題則很大程度上是嶄新的。

以下舉幾種簡化場景爲例：

1. 訓練數據均屬於 public domain[^4]，或以寬鬆（permissive）自由許可證[^5]授權的作品
2. 訓練數據除了上述來源以外，還含有以 copyleft 自由許可證授權的作品
3. 訓練數據含有其他類型的許可，但在訓練及分發模型過程中遵守了所有的授權條件
4. 訓練或分發模型的過程本身就違反了源數據的授權條件

[^4]: 中文常譯爲「公有領域」或「公衆領域」。

[^5]: 自由許可證通常定義爲在使用、修改、複製分發原作、分享經過修改或衍生的作品這四個方面授予使用者自由的許可證。自由許可證可以附加一些與上述自由不衝突的條件或聲明，例如署名／姓名標示（attribution）要求等。特別地，有些自由許可證要求在分享經過修改或衍生的作品時，也必須以同樣或類似的許可證進行，從而使得這些自由可以傳遞下去，這樣的自由許可證稱爲 copyleft 許可證；而另一些則無此附加要求，稱爲寬鬆許可證。

第一種情形下通常爭議最少。假設採取「訓練後的模型是訓練數據的衍生作品」的觀點，需要做的也只是遵守對應的寬鬆許可證，並不困難。但即使這種最簡單的情形，關於運行輸出的問題也並非顯然。當訓練數據的授權條件包含「署名／姓名標示」時，假設採取「模型的輸出也是訓練數據的衍生作品」的觀點，則要想分享任何模型輸出，就可能需要爲其標示來源。但來自源數據的信息早已在訓練階段「混合」，因此對輸出的標示只能和模型本體一樣，羅列全部數據的來源。有些觀點可能認爲這仍不滿足署名／姓名標示要求。[^6]

[^6]: <https://jillianbommarito.com/train-llm-cc-by-sa/> 這篇雖然主要分析 CC BY-SA（屬於 copyleft 許可證），但關於署名／姓名標示條款的討論同樣適用於寬鬆許可證。其中提到此問題的一種可能的解決思路：有署名要求的作品不用作訓練數據，只作爲 RAG 數據庫使用。

接下來看第二種情形。與第一種情形的不同在於，不同 copyleft 許可證的作品常常是無法混合的。也就是說，要想在遵守所有許可證的前提下分享訓練後的模型，訓練數據中最多只能加入以一種 copyleft 許可證授權的作品（例如只加入以 GPLv3 授權的作品），除非許可證中有明確的兼容性條款[^7]。當然，到模型輸出的署名／姓名標示方面，也面臨着一樣的問題。

[^7]: 有名的兼容性條款例子：CC BY-SA 4.0 允許衍生作品在 GPLv3 下發佈，因此，可以將以 CC BY-SA 4.0 授權的作品 A 和以 GPLv3 授權的作品 B 整合爲一件新作品，並在 GPLv3 下發佈。

第三種其實包括了無數複雜情形，惟和本篇主題關聯不大，略過不作討論。（打賭沒人會讀到這裏……）

關於第四種情形，這裏只取一個很小的切入點，即問：從模型的利用者的角度出發，我們可以藉助哪些做法，減少使用這些並非基於同意或授權而煉成的模型。經驗上，當我們不確定一件工具是否足夠可靠時，容易想到這樣幾個方向：保留備選項，避免依賴單一工具；繼續獲取信息，變未知爲已知；乃至參與製造工具，自助互助。這些策略所需資源的種類與數量各不相同，並不一定都可行。就目前的 LLM 領域而言，完全公開訓練數據的模型相當少見，而自行訓練亦非大多數人所可及。只好坦承，具體可執行的方案既超出了本文的範疇，也超出了我的能力，也許熟悉這一塊的讀者可以補充。

### 公平性

TODO

### 其他可能带来的變化

TODO
